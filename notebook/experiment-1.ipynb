{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3309828d-0e8c-4457-98ff-6738a30c5791",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df0bfe6-4ff6-435a-9f42-04b625ba8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_filepath = 'PDFs/1.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1968e5-cf46-42cb-8cd4-82d0e9aac66e",
   "metadata": {},
   "source": [
    "## Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69ad58e-24ca-4e1c-ad7f-55edcd6bf3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c957e010-1897-4507-b0e0-4b24f6c340a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(filepath):\n",
    "    text = extract_text(filepath)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a18416-fbe4-44f6-b0eb-1149a7d76fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = extract_text_from_pdf(sample_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80929d83-a57b-4e13-8055-ef132ab0fc36",
   "metadata": {},
   "source": [
    "# Data Prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb8848-d605-4462-b4f1-84f388c6d915",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5535d63c-ac03-4748-a13f-346c0c443265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a3feca9-71ea-4593-9d4c-cf36ba067752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "244a7c27-b17c-4896-bf0c-2bb831f0051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "\n",
    "    processed_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.ent_type_ in ('PERSON', 'ORG', 'GPE', 'LOC'):\n",
    "            processed_tokens.append(token.text)\n",
    "        else:\n",
    "            processed_tokens.append(token.text.lower())\n",
    "\n",
    "    text = ' '.join(processed_tokens)\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab41b7f-d21a-4bcb-9002-3457c2ef0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = preprocess_text(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e54128-0c39-4350-85c7-5364f4d0820c",
   "metadata": {},
   "source": [
    "## Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a46b2b59-c1ba-4789-a967-7a707593213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e00a946-e542-4515-b953-c4a1b44574dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akash\\Desktop\\GenAI\\Text Summarizer\\Text-Summarizer\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d808c3-b684-4c05-aa02-a1bf6db63da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(preprocessed_text)\n",
    "sentence_embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be0687b-61f0-47bd-9af3-ed475250c9a6",
   "metadata": {},
   "source": [
    "## Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb7064b1-4f9d-4f58-8ab6-9c66cf2369bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ee5c4ee-67eb-41a3-8f00-b946f226cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, n_init='auto', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34e55d90-56a8-43fc-8786-b24ebaf76d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=5, n_init=&#x27;auto&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=5, n_init=&#x27;auto&#x27;, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=5, n_init='auto', random_state=42)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.fit(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2abfbede-8ff8-4413-839f-5a3cc42ccbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "\n",
    "for idx, label in enumerate(kmeans.labels_):\n",
    "    if label not in clusters:\n",
    "        clusters[label] = []\n",
    "    clusters[label].append(sentences[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6799652f-6b54-4a66-96bf-280713a88bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 4:\n",
      " - Journal Parallel distributed computing 130 ( 2019 ) 1223 content list available ScienceDirect J .\n",
      " - journal homepage : www.elsevier.com/locate/jpdc security machine learning adversarial setting : survey Xianmin Wang , , Jing Li , Xiaohui Kuang b , Yu - Tan c , Jin Li , School Computer Science , Guangzhou University , Guangzhou 510006 , China b National Key Laboratory Science Technology Information System Security , Beijing , China c School Computer Science , Beijing Institute Technology University , Beijing , China State Key Laboratory Information Security , Institute Information Engineering , Chinese Academy Sciences , Beijing , China r c l e n f b r c article history : received 17 december 2018 received revised form 2 february 2019 accepted 2 march 2019 available online 3 april 2019 keywords : machine learning adversarial setting adversarial attack adversarial example security model machine learning ( ML ) method demonstrated impressive performance many application field autopilot , facial recognition , spam detection .\n",
      " - introduction past decade , machine learning ( ML ) [ 42 ] powered many aspect practical task , e.g .\n",
      " - e - mail address : xianmin @ gzhu.edu.cn ( X. Wang ) , lijing @ gzhu.edu.cn ( J. Li ) , xiaohui_kuang @ 163.com ( X. Kuang ) , tan2008 @ bit.edu.cn ( y.-a .\n",
      " - Tan ) , jinli71 @ gmail.com ( J. Li ) .\n",
      " - http : //doi.org/10.1016/j.jpdc.2019.03.003 0743 - 7315/ 2019 elsevier inc. right reserved .\n",
      " - / Journal Parallel distributed computing 130 ( 2019 ) 1223 13 Fig .\n",
      " - internet thing ( iot ) , smart home system , wireless sensor network , voice video traffic , mobile platform informa- tion security [ 46,51,86,105 ] .\n",
      " - combined SL USL method , RL enabled computer defeat human go champion 2016 [ 87 ] .\n",
      " - / Journal Parallel distributed computing 130 ( 2019 ) 1223 Fig .\n",
      " - 2014 , improved ML security framework [ 7 ] .\n",
      " - / Journal Parallel distributed computing 130 ( 2019 ) 1223 15 Fig .\n",
      " - currently , deep learning ( dl ) technique increasingly rais- ing concern academic industrial community [ 26,107 , 109 ] .\n",
      " - / Journal Parallel distributed computing 130 ( 2019 ) 1223 table 1 summary main attribute different attack paper .\n",
      " - / Journal Parallel distributed computing 130 ( 2019 ) 1223 17 Fig .\n",
      " - / Journal Parallel distributed computing 130 ( 2019 ) 1223 table 2 summary main attribute different defense method paper .\n",
      " - / Journal Parallel distributed computing 130 ( 2019 ) 1223 19 Fig .\n",
      " - / Journal Parallel distributed computing 130 ( 2019 ) 1223 Fig .\n",
      " - / Journal Parallel distributed computing 130 ( 2019 ) 1223 21 Fig .\n",
      " - conclusion ml method widely employed various field internet thing , medical imaging , computer vision , social networking [ 106 ] .\n",
      " - acknowledgment work supported National Natural Science Foundation China ( no .\n",
      " - 61472091 61370194 ) , Natural Science Foundation Guangdong Province , China distin- guished Young Scholars ( .\n",
      " - 2014a030306020 ) , Guangzhou Scholars Project Universities Guangzhou , China ( .\n",
      " - 1201561613 ) , Science Technology Planning Project Guangdong Province , China ( .\n",
      " - 2015b010129015 ) , National Natural Science Foundation Outstanding Youth Foundation , China ( .\n",
      " - 61722203 ) , National Key R & D Program China ( .\n",
      " - 2016yfn0800602 ) , Shandong Provincial Key R & D Pro- gram China ( .\n",
      " - reference [ 1 ] a. adler , vulnerability biometric encryption system , : international conference audio- video - based biometric person authentication , 2005 , pp .\n",
      " - [ 2 ] n. akhtar , J. Liu , A. Mian , Defense Against Universal Adversarial Perturbations , 2018 .\n",
      " - Pino , F. Lasheras , Attitude estima- tion based array passive RFID tag , IEEE Trans .\n",
      " - [ 5 ] b. biggio , I. Corona , G. Fumera , G. Giacinto , F. Roli , Bagging Classifiers fighting poisoning attack adversarial classification task , Springer Berlin Heidelberg , 2011 .\n",
      " - / Journal Parallel distributed computing 130 ( 2019 ) 1223 [ 6 ] b. biggio , G. Fumera , F. Roli , multiple classifier system robust classifier design adversarial environment , Int .\n",
      " - Fan , M. Lai , Y. Xu , unsupervised learning cell - level visual representation generative adversarial network , ieee J. Biomed .\n",
      " - : proceeding 2013 acm workshop Artificial Intelligence Security , 2013 , pp .\n",
      " - : proceeding 2013 acm workshop artificial intelligence security , ACM , 2013 , pp .\n",
      " - [ 12 ] b. biggio , K. Rieck , d. ariu , C. Wressnegger , I. Corona , G. Giacinto , F. Roli , poisoning behavioral malware clustering , : proceeding 2014 workshop artificial intelligent security workshop , ACM , 2014 , pp .\n",
      " - [ 13 ] b. biggio , F. Roli , Wild Patterns : ten year rise adversarial machine learning , 2017 .\n",
      " - Bojnordi , e. ipek , Memristive Boltzmann machine : hardware accelerator combinatorial optimization deep learning , : ieee international symposium high performance computer architecture , 2016 , pp .\n",
      " - [ 15 ] n. carlini , d. wagner , towards evaluating robustness neural network , : security privacy , 2017 , pp .\n",
      " - [ 16 ] n. dalvi , P. Domingos , Mausam , S. Sanghai , D. Verma , adversarial classi- fication , : Tenth ACM SIGKDD International Conference Knowledge Discovery Data Mining , 2004 , pp .\n",
      " - Kounavis , D.H. Chau , keeping bad guy : protecting vaccinating deep learning jpeg compression , 2017 .\n",
      " - Li , ImageNet : large - scale hierarchical image database , : Computer Vision pattern recognition , 2009 .\n",
      " - ieee conference , 2009 , pp .\n",
      " - Ran , k. laine , k. lauter , M. Naehrig , J. Wernsing , cryp- tonets : applying neural network encrypted data high throughput accuracy , : radio wireless symposium , 2016 , pp .\n",
      " - [ 20 ] c. dwork , differential privacy , : International Colloquium Automata , Languages , Programming , 2006 , pp .\n",
      " - Roy , study effect jpg compression adversarial image , 2016 .\n",
      " - [ 23 ] k. eykholt , I. Evtimov , E. Fernandes , l. bo , d. song , Robust Physical - World Attacks Deep Learning Visual Classification , 2018 .\n",
      " - [ 24 ] f. falla - moya , F. Torres - Rojas , object recognition using hierarchical temporal memory , : international symposium intelligent computing system , 2018 .\n",
      " - Beam , Adversarial attack medical deep learning system , 2018 , arxiv preprint arxiv:1804.05296 .\n",
      " - [ 27 ] [ 26 ] C. Gao , S. Lv , Y. Wei , Z. Wang , Z. Liu , X. Cheng , - sse : effective searchable symmetric encryption enhanced security mobile device , IEEE Access 6 ( 2018 ) 3886038869 .\n",
      " - J. Gao , B. Wang , Z. Lin , W. Xu , Y. Qi , DeepCloak : masking deep neural network model robustness adversarial sample , 2017 .\n",
      " - [ 28 ] m. gardner , J. Grus , m. neumann , O. Tafjord , P. Dasigi , N. Liu , M. Peters , M. Schmitz , L. Zettlemoyer , AllenNLP : deep semantic natural language processing platform , 2018 .\n",
      " - J. Shlens , C. Szegedy , explaining harnessing [ 29 ] [ 30 ] [ 31 ] k. grosse , N. Papernot , P. Manoharan , M. Backes , P. Mcdaniel , adversarial example malware detection , 2017 , pp .\n",
      " - [ 32 ] t. gu , B. Dolan - Gavitt , s. garg , badnets : identifying vulnerability machine learning model supply chain , 2017 , arxiv preprint arxiv : 1708.06733 .\n",
      " - maaten , countering adversarial image using Input Transformations , 2017 .\n",
      " - Sun , deep residual learning image recog- nition , : ieee conference computer vision pattern recognition , 2016 , pp .\n",
      " - [ 41 ] a. kurakin , I. Goodfellow , S. Bengio , Adversarial Machine Learning Scale , 2016 .\n",
      " - [ 42 ] y. lecun , Y. Bengio , G. Hinton , deep learning , nature 521 ( 7553 ) ( 2015 ) 436 .\n",
      " - [ 44 ] h. lee , S. Han , J. Lee , generative adversarial trainer : defense adversarial perturbation GAN , 2017 .\n",
      " - [ 45 ] m. leece , unsupervised learning htns complex adversarial domain , : thirtieth aaai conference artificial intelligence , 2016 .\n",
      " - J. Li , L. Sun , Q. Yan , Z. Li , W. Srisa - , H. Ye , Significant permission identification machine learning based android malware detection , ieee Trans .\n",
      " - [ 49 ] z. liang , H. Chen , J. Zhu , K. Jiang , Y. Li , adversarial deep reinforcement learning portfolio management , Papers ( 2018 ) .\n",
      " - Tan , X. Zhang , X. Wang , J. Zheng , Q. Zhang , building packet length covert channel mobile VoIP traffic , J. Netw .\n",
      " - Tan , payload- dependent packet rearranging covert channel mobile VoIP traffic , inform .\n",
      " - [ 52 ] Y. Liu , X. Chen , C. Liu , D. Song , delving transferable adversarial example black - box attack , 2016 .\n",
      " - [ 53 ] x. liu , L. Qin , P. Tao , w. based personal health record ( phr ) system , s0020025516304571 .\n",
      " - 379 ( 2016 ) [ 54 ] q. liu , G. Wang , X. Liu , T. Peng , J. Wu , achieving reliable secure service cloud computing environment , Comput .\n",
      " - [ 55 ] d. lowd , C. Meek , adversarial learning , : Eleventh ACM SIGKDD in- ternational conference knowledge discovery Data Mining , 2005 , pp .\n",
      " - Liang , unified gradient regularization family adversarial example , : IEEE International Conference Data Mining , 2016 , pp .\n",
      " - moosavi - dezfooli , a. fawzi , O. Fawzi , p. frossard , universal adver- sarial perturbation , : ieee conference computer vision pattern Recognition , 2017 , pp .\n",
      " - Moosavidezfooli , A. Fawzi , O. Fawzi , p. frossard , Universal Adversarial Perturbations , 2017 , pp .\n",
      " - Moosavidezfooli , A. Fawzi , p. frossard , deepfool : simple accurate method fool deep neural network , : Computer Vision Pattern Recognition , 2016 , pp .\n",
      " - [ 62 ] l. nguyen , S. Wang , A. Sinha , learning masking approach secure learning , 2018 .\n",
      " - [ 63 ] a. nguyen , J. Yosinski , J. Clune , Deep Neural Networks easily fooled : high confidence prediction unrecognizable image , 2015 , pp .\n",
      " - [ 64 ] n. papernot , characterizing Limits Defenses Machine Learning adversarial setting , 2018 .\n",
      " - [ 65 ] n. papernot , P. Mcdaniel , I. Goodfellow , Transferability Machine Learn- ing : Phenomena Black - Box Attacks using Adversarial Samples , 2016 .\n",
      " - Celik , A. Swami , limitation deep learning adversarial setting , : ieee european symposium security privacy , 2016 , pp .\n",
      " - Dean , distilling knowledge neural [ 68 ] n. papernot , P. Mcdaniel , a. sinha , M. Wellman , towards science network , comput .\n",
      " - security privacy Machine Learning , 2016 .\n",
      " - / Journal Parallel distributed computing 130 ( 2019 ) 1223 23 [ 69 ] n. papernot , P. Mcdaniel , X. Wu , S. Jha , a. swami , Distillation defense adversarial perturbation deep neural network , : security Privacy , 2016 , pp .\n",
      " - [ 70 ] n. papernot , S. Song , I. Mironov , a. raghunathan , K. Talwar , lfar erlingsson , scalable private learning pate , 2018 .\n",
      " - [ 71 ] l. pinto , J. Davidson , R. Sukthankar , A. Gupta , Robust Adversarial Reinforcement Learning , 2017 .\n",
      " - [ 74 ] L. Qin , G. Wang , L. Feng , S. Yang , w. jie , preserving privacy probabilistic indistinguishability weighted social network , IEEE Trans .\n",
      " - Dertouzos , data bank privacy homomorphism , Found .\n",
      " - [ 77 ] c. robert , Machine Learning , probabilistic perspective , Taylor & Francis , 2014 .\n",
      " - [ 78 ] A.S. Ross , F. Doshivelez , improving adversarial robustness Interpretability Deep Neural Networks regularizing input gradient , 2017 .\n",
      " - Tygar , ANTIDOTE : understanding defending poison- ing anomaly detector , : acm sigcomm conference internet measurement 2009 , Chicago , Illinois , Usa , november , 2009 , pp .\n",
      " - Lim , Regularizing Deep network using efficient layerwise adversarial training , 2017 .\n",
      " - [ 81 ] s. sarkar , a. bansal , U. Mahbub , R. Chellappa , UPSET angri : breaking high performance image classifier , 2017 .\n",
      " - [ 82 ] l. schmidt , S. Santurkar , D. Tsipras , K. Talwar , A. Mdry , adversarially robust generalization requires data , 2018 .\n",
      " - [ 85 ] s. shen , G. Jin , K. Gao , Y. Zhang , APE - GAN : adversarial perturbation [ 86 ] elimination GAN , 2017 .\n",
      " - J. Shen , C. Wang , T. Li , X. Chen , X. Huang , z.-h. Zhan , secure data uploading scheme smart home system , inform .\n",
      " - , mastering game go deep neural network tree search , nature 529 ( 7587 ) ( 2016 ) 484 .\n",
      " - [ 88 ] H. Simon , neural network : A Comprehensive Foundation , Prentice Hall PTR , 1994 , pp .\n",
      " - [ 89 ] c. smutz , a. stavrou , malicious PDF detection using metadata structural feature , : computer security application conference , 2012 , pp .\n",
      " - [ 90 ] y. song , T. Kim , S. Nowozin , S. Ermon , N. Kushman , PixelDefend : lever- aging Generative Models Understand Defend adversarial example , 2017 .\n",
      " - [ 91 ] n. rndic , P. Laskov , Detection malicious pdf file based hierarchical : proceeding 20th annual network & document structure , distributed system security symposium , 2013 , pp .\n",
      " - [ 92 ] su , xiaogang , Linear Regression Analysis , World Scientific , 2009 , pp .\n",
      " - , reinforcement learning : introduction , MIT press , 1998 .\n",
      " - [ 94 ] c. szegedy , V. Vanhoucke , s. J. Shlens , Z. Wojna , rethinking inception architecture computer vision , Comput .\n",
      " - Ioffe , [ 95 ] c. szegedy , W. Zaremba , I. Sutskever , J. Bruna , d. erhan , I. Goodfellow , r. fergus , intriguing property neural network , Comput .\n",
      " - [ 96 ] f. tramr , A. Kurakin , N. Papernot , I. Goodfellow , D. Boneh , P. Mcdaniel , ensemble adversarial training : attack Defenses , 2017 .\n",
      " - Reiter , T. Ristenpart , stealing machine learning model via prediction apis , 2016 .\n",
      " - [ 98 ] Y. Wang , C. Ding , Z. Li , G. Yuan , S. Liao , X. Ma , B. Yuan , X. Qian , J. Tang , Q. Qiu , towards ultra - high performance energy efficiency deep learning system : algorithm - hardware co - optimization framework , 2018 .\n",
      " - [ 100 ] h. xiao , h. xiao , C. Eckert , adversarial label flip attack support vector machine , : european conference artificial intelligence , 2012 , pp .\n",
      " - human splicing code reveals new insight genetic determinant disease , science 347 ( 6218 ) ( 2015 ) 1254806 .\n",
      " - [ 102 ] W. Xu , d. evans , Y. Qi , feature squeezing : detecting adversarial example deep neural network , : network distributed system security symposium , 2018 .\n",
      " - [ 103 ] W. Xu , Y. Qi , d. evans , automatically evading classifier : case study PDF malware classifier , : network distributed system security symposium , 2016 .\n",
      " - [ 104 ] C. Yan , W. Wei , X. Zhang , randomizing SVM adversarial attack uncertainty , : Pacific - Asia Conference Knowledge Discovery Data Mining , 2018 .\n",
      " - [ 106 ] s. zhang , L. Qin , Y. Lin , anonymizing popularity online social network full utility , Future Gener .\n",
      " - Long , J. Yin , Z. Cai , G. Xia , sampling attack active : international conference learning adversarial environment , Modeling Decisions Artificial Intelligence , Springer , 2012 , pp .\n",
      " - Tan , L. Zhu , X. Wang , Q. Zhang , Y. Li , identity - based anti - quantum privacy - preserving blind authentication wireless sensor network , sensor 18 ( 5 ) ( 2018 ) 1663 .\n",
      " - degree Suzhou University , Jiangsu , China , 2006 , M.S .\n",
      " - degree computer science Jiangxi University Science Technology , Jiang Xi , China , 2013 .\n",
      " - received ph.d. degree computer science 2017 Beihang University .\n",
      " - currently , working institution School computer science Guangzhou university .\n",
      " - degree Inner Mongol nor- mal University 2010 , M.S .\n",
      " - degree Shaanxi Normal University 2013 ph.d. degree Beijing University Posts Telecommunications .\n",
      " - Xiaohui Kuang currently deputy director na- tional Key Laboratory Science Technology information system security , Beijing , China .\n",
      " - Yu - Tan currently professor ph.d. supervisor Beijing Institute Technology , senior member China Computer Federation .\n",
      " - Jin Li currently professor vice dean School Computer Science , Guangzhou University .\n",
      " - ( 2004 ) Southwest univer- sity Sun Yat - sen University , Mathematics .\n",
      " - got ph.d. degree information security Sun Yat - sen University 2007 .\n",
      " - research interest include design secure protocol Computing privacy protection various new computing environ- ments , including Cloud computing , internet - - thing , Blockchain machine learning .\n",
      "\n",
      "Cluster 2:\n",
      " - Parallel Distrib .\n",
      " - comput .\n",
      " - 1 .\n",
      " - Fig .\n",
      " - 1 ( ) ) .\n",
      " - 1 ( b ) ) .\n",
      " - specifically , Szegedy et al .\n",
      " - 1 .\n",
      " - environment whereby ML model deployed .\n",
      " - 2 ) .\n",
      " - 3 .\n",
      " - 2 .\n",
      " - Fig .\n",
      " - 2 .\n",
      " - Fig .\n",
      " - 3 .\n",
      " - 3.2 .\n",
      " - , lowd et al .\n",
      " - addition , Kurakin et al .\n",
      " - 4 .\n",
      " - 1 ( b ) ) .\n",
      " - 3.3 .\n",
      " - 4 .\n",
      " - Fig .\n",
      " - 5 .\n",
      " - Fig .\n",
      " - 4 .\n",
      " - example , Wittel et al .\n",
      " - srndic et al .\n",
      " - based fact , Szegedy et al .\n",
      " - 4.1 .\n",
      " - eq .\n",
      " - Eq .\n",
      " - Fig .\n",
      " - 6 .\n",
      " - however , application ( e.g .\n",
      " - Fig .\n",
      " - biggio et al .\n",
      " - addition , Xiao et al .\n",
      " - 4.2 .\n",
      " - 7 .\n",
      " - smaller lead le perceivable perturbation image .\n",
      " - [ 81 ] .\n",
      " - 5 .\n",
      " - 5.1 .\n",
      " - 8 .\n",
      " - 8 ( ) ) .\n",
      " - ( Fig .\n",
      " - 8 ( b ) ) .\n",
      " - 5.2 .\n",
      " - Nelson et al .\n",
      " - 5.3.1 .\n",
      " - moreover , Guo et al .\n",
      " - da et al .\n",
      " - addition , moosavi et al .\n",
      " - foveation - based method Boix et al .\n",
      " - Fig .\n",
      " - 8 .\n",
      " - [ 79 ] .\n",
      " - 5.3 .\n",
      " - 9 .\n",
      " - Fig .\n",
      " - 10 .\n",
      " - example , Lyu et al .\n",
      " - Shaham et al .\n",
      " - Fig .\n",
      " - depicted Fig .\n",
      " - 10 .\n",
      " - 5.3.2 .\n",
      " - , Lee et al .\n",
      " - 11 ) .\n",
      " - feature squeezing method Evans et al .\n",
      " - framework feature squeezing method depicted Fig .\n",
      " - 12 .\n",
      " - 11 .\n",
      " - Fig .\n",
      " - 12 .\n",
      " - Fig .\n",
      " - depicted Fig .\n",
      " - approach ( e.g .\n",
      " - approach ( e.g .\n",
      " - 5.4 .\n",
      " - 5.4.1 .\n",
      " - recently , papernot et al .\n",
      " - Fig .\n",
      " - 5.4.2 .\n",
      " - 13 .\n",
      " - Fig .\n",
      " - 14 .\n",
      " - yao et al .\n",
      " - dowlin et al .\n",
      " - 6 .\n",
      " - learn .\n",
      " - J. Mach .\n",
      " - Learn .\n",
      " - knowl .\n",
      " - Data Eng .\n",
      " - knowl .\n",
      " - Data Eng .\n",
      " - Learn .\n",
      " - re .\n",
      " - Chang , Y .\n",
      " - Health Inform .\n",
      " - surv .\n",
      " - Learn .\n",
      " - re .\n",
      " - 2009 .\n",
      " - 113 .\n",
      " - 112 .\n",
      " - J. Mach .\n",
      " - Learn .\n",
      " - re .\n",
      " - Kohane , A.L .\n",
      " - I.J .\n",
      " - Neural Inf .\n",
      " - process .\n",
      " - syst .\n",
      " - I.J .\n",
      " - sci .\n",
      " - syst .\n",
      " - appl .\n",
      " - comput .\n",
      " - appl .\n",
      " - Neurosci .\n",
      " - Ind .\n",
      " - Inf .\n",
      " - comput .\n",
      " - appl .\n",
      " - sci .\n",
      " - sci .\n",
      " - electr .\n",
      " - eng .\n",
      " - sci .\n",
      " - sci .\n",
      " - Learn .\n",
      " - Technol .\n",
      " - sci .\n",
      " - Technol .\n",
      " - Parallel Distrib .\n",
      " - syst .\n",
      " - learn .\n",
      " - Secure Comput .\n",
      " - 114 .\n",
      " - Technol .\n",
      " - re .\n",
      " - Dev .\n",
      " - sci .\n",
      " - sci .\n",
      " - 116 .\n",
      " - Sutton , A.G. Barto , et al .\n",
      " - sci .\n",
      " - sci .\n",
      " - wittel , S.F .\n",
      " - comput .\n",
      " - appl .\n",
      " - comput .\n",
      " - syst .\n",
      " - comput .\n",
      " - appl .\n",
      "\n",
      "Cluster 3:\n",
      " - traditionally , ML model trained deployed benign setting , testing training data identical statistical characteristic .\n",
      " - work , present comprehensive overview investigation security property ML algorithm adversarial setting .\n",
      " - first , analyze ML security model develop blueprint interdisciplinary research area .\n",
      " - finally , relying upon reviewed work , provide prospective relevant future work designing secure ML model .\n",
      " - ml method representation - learning method map input domain output domain observing sample pair input output .\n",
      " - feature computed machine usually robust representative hand - crafted feature , ML method achieve satisfactory result many application .\n",
      " - ML method traditionally developed assump- tion training evaluating process ML implemented benign setting .\n",
      " - result , statistical characteristic data employed make prediction statistical characteristic data used training ML model .\n",
      " - however , assumption usually hold corresponding author .\n",
      " - 1 demonstrates two setting whereby ML model deployed .\n",
      " - implies source data traditional statistical model ML algorithm benign setting depend classifier , noise affecting data stochastic ( Fig .\n",
      " - continuing advancement deep learning ( dl ) method [ 34,94 ] high accessibility hardware train- ing complex model [ 14,98 ] , ML method advanced new stage : provided naturally occurring input , outperform human being several task .\n",
      " - small perturbation carefully crafted X. Wang , J. Li , X. Kuang et al .\n",
      " - ( ) represents benign setting , ( b ) represents adversarial setting .\n",
      " - learning ( sl ) method , unsupervised learning ( USL ) method reinforcement learning ( rl ) method .\n",
      " - work , present comprehensive survey regarding security ML adversarial setting context various application .\n",
      " - finally discus future research challenge give wide viewpoint research direction conclusion section .\n",
      " - training phase SL method , input data la- beled corresponding output .\n",
      " - typical instance SL method include decision tree ( dts ) [ 75 ] , support vector ma- chine ( SVM ) [ 35 ] , neural network ( nns ) [ 88 ] linear regres- sion ( lr ) [ 92 ] .\n",
      " - many practical task based SL method , e.g .\n",
      " - , natural language processing ( NLP ) [ 28 ] , object recognition image [ 24 ] , Attitude Estimation [ 3 ] Computer Aided trans- lation ( CAT ) [ 83 ] .\n",
      " - usl method consist model pre - training [ 22 ] , clustering point [ 39 ] dimensionality reduction [ 40 ] .\n",
      " - rl method discover optimal agent behavior trial- - error process based action , observation reward agent [ 37,93 ] .\n",
      " - subfield controlling scheming .\n",
      " - since rl method usually associated SL USL technique , susceptible adversarial example adversarial setting [ 49,71 ] .\n",
      " - overview machine learning 3.1 .\n",
      " - machine learning attack surface ML interdisciplinary area computer science , statistic , probability brain - like technology , therein playing vital role AI research .\n",
      " - ML algorithm used designing imple- menting automatic knowledge acquisition system imitating human behavior .\n",
      " - , supervised measure security ML system , paperno et al .\n",
      " - note top line generalized data processing 14 X. Wang , J. Li , X. Kuang et al .\n",
      " - shown , data collection process , adversary access ML model manipulating data acquisition process .\n",
      " - dataset training , feature extraction learning stage ML easily influenced adversarial example .\n",
      " - proposed idea adversarial learning [ 55 ] motivated Dalvis work .\n",
      " - explicitly presented concept ML security , well taxonomy modeling attack behavior [ 4 ] .\n",
      " - based study , taxonomy categorizing ML threat model classified along three direction , shown Fig .\n",
      " - adversarial model machine learning ML adversarial model includes adversarial goal , adver- sarial capability , adversarial knowledge adversarial strat- egy .\n",
      " - knowledge includes limited knowledge perfect knowledge based specific constraint ML model .\n",
      " - specific ML constraint consist training set , model parameter , feedback information , etc .\n",
      " - security level determined range adversary access ML system .\n",
      " - 5 illustrates adver- sarial capability different phase ML system .\n",
      " - ( 4 ) attack strategy .\n",
      " - important part DL , deep neural network ( dnns ) achieve remarkable result many multi - mode recognition task .\n",
      " - case , dl method outperform traditional ML method term prediction accuracy ; however , Szegedy et al .\n",
      " - Subsequently , research team also developed numer- ous attack approach popular dl - based system , e.g .\n",
      " - note earlier work primarily concerned traditional learning method , e.g .\n",
      " - , SVM , NB DT , whereas current work focus dl method .\n",
      " - table 1 provides summary main attribute attack mentioned paper .\n",
      " - support ic 2 represented vector .\n",
      " - 2 min k k2 ( 1 ) 2 h represents classification model dnn l label input image , contrast origin label ic .\n",
      " - ( 1 ) non - trivial , approximate solution box - constrained l - bfgs recommended .\n",
      " - specif- ically , equation solved finding minimum h > 0 following problem : l ( ic + min , l ) | | + ) ( 2 ) l ; 2 h 2 ) [ 0 , 1 ] h ( ic + eq .\n",
      " - ( 2 ) satisfy ic + l loss function classification .\n",
      " - fast gradient sign method ( FGSM ) FGSM efficient adversarial generation method pro- posed Goodfellow [ 96 ] .\n",
      " - = l , ( 3 ) `` sign ( & ( , ic , l ) ) r r = & cost gradient regard ic parameter , sign symbol represents sign operator , scaler value `` used confine volume perturbation .\n",
      " - note higher dimensional space , FGSM example present linearity characteristic dnn model , whereas dnn model generally considered highly non - linear .\n",
      " - de- ceive dnn model every image high confidence , moosavi - dezfooli et al .\n",
      " - suppose original image col- lected distribution d. universal perturbation determined ( h ( ic ) h ( ic + 6= ) ) \u0000 \u0000 p ic 2 ( 4 ) Fig .\n",
      " - attack method roughly comparable appending random noise original dataset .\n",
      " - , k kp \u0000 represents attack ratio , kkp lp - norm , predefined constant .\n",
      " - note perturbation computed algorithm generalized well various network , whereas moosavi - dezfooli et al .\n",
      " - validated effectiveness proposed method ResNet .\n",
      " - process interpreted following optimization equation .\n",
      " - = max ( min ( kN ( ) ic , 1 ) , 1 ) + ( 5 ) N ( ) denotes perturbation generated resid- ual generating network n input data i. contrast UPSET method , ANGRI algorithm computes specific perturbation based original work .\n",
      " - method reported achieve favorable performance fooling DL model cifar-10 mnist datasets .\n",
      " - compared previous adversarial example creation meth- od , C & W attack achieve better performance term computation speed .\n",
      " - moreover , experimental result demonstrate one com- mon property adversarial example transfer well different neural network relatively similar architec- ture .\n",
      " - defense strategy defense strategy adversarial setting includes four main part : ( 1 ) security evaluation mechanism ML model , ( 2 ) defense strategy training phase , ( 3 ) defense strategy prediction / test phase , ( 4 ) privacy protection method ML system .\n",
      " - moreover , table 2 summarizes main attribute defense method presented paper .\n",
      " - address problem , various study proposed number method evaluating security robustness ML model [ 7,8 ] .\n",
      " - Charac- terized defense pattern , mechanism underlying evaluation method categorized reactive defense proactive defense , shown Fig .\n",
      " - finally , designer add feature defend novel attack update ML system ( Fig .\n",
      " - resist poisoning attack , common method enhance ML model robustness sanitize training data .\n",
      " - method first build new dataset appending candidate sample original training dataset train ML model using new dataset ; finally , ML model 18 X. Wang , J. Li , X. Kuang et al .\n",
      " - Defense Strategies Reactive / Proactive Representative Studies Defense strategy training phase data sanitization generalization enhancing defense strategy prediction phase adversarial training data compression foveation - based method gradient masking defensive distillation DeepCloak GAN - based method Feature squeezing universal defense method dp - based method - based method privacy protection strategy Reactive Proactive Proactive Reactive Reactive Reactive proactive reactive proactive reactive proactive proactive proactive [ 61 ] [ 5,6,79 ] [ 30,60,80 ] [ 17,21,33,59 ] [ 56 ] [ 57,62,78,84 ] [ 36,69 ] [ 27 ] [ 44,85 ] [ 102 ] [ 2 ] [ 70 ] [ 19,53,54,76 ] tivation function .\n",
      " - method change ML model parameter .\n",
      " - since discovery adversarial attack ML model , general consensus adversar- ial training enhance robustness adversarial example .\n",
      " - contrast general ML training process , adversarial training incorporates hybrid dataset including adversarial example original sample .\n",
      " - essence , adversarial training robust generalization method ML training phase [ 30,60 ] .\n",
      " - data compression method Ghahramani et al .\n",
      " - observed widely used ML model datasets comprised jpg format image .\n",
      " - per- formed research regard impact JPG com- pression [ 21 ] .\n",
      " - studied image JPEG compression effect mitigate impact adver- sarial image [ 17,33 ] .\n",
      " - proposed use discrete cosine transform ( dct ) compression defend universal perturba- tions [ 59 ] .\n",
      " - observed dnn model robust scale transformation change original image enforced foveation approach , whereas property generalize adversarial pattern .\n",
      " - however , effectiveness foveation powerful attack validated .\n",
      " - conceptual defense [ 13 ] , ( ) reactive defense ( b ) proactive defense .\n",
      " - representation reactive defense proactive classification performance trained original dataset new dataset evaluated .\n",
      " - another defense strategy mitigates impact adversarial example improving generalization capability ML model .\n",
      " - representation method include Bagging ( boot- strap aggregating algorithm ) RSM ( random subspace method ) method proposed biggio et al .\n",
      " - [ 5,6 ] ANTIDOTE algo- rithm presented rubinstein et al .\n",
      " - via transforming training data/ process , adjusting network layer , changing ac- X. Wang , J. Li , X. Kuang et al .\n",
      " - main concept DeepCloak technique [ 27 ] .\n",
      " - gradient masking method gradient masking method enhance ML robustness mod- ifying gradient input data , loss / activation func- tion , etc .\n",
      " - attempted increase local stability neural network min- imizing loss function adversarial example model parameter updating [ 84 ] .\n",
      " - addition , Ross Doshi - Velez analyzed gradient regularization input pro- posed method improving network robustness [ 78 ] .\n",
      " - defensive distillation method Hinton et al .\n",
      " - observed knowledge large DDN model transferred small model distillation technique [ 36 ] .\n",
      " - motivated work , Papernot et al .\n",
      " - 9 demonstrates framework defensive distillation method .\n",
      " - 9 , dnn model first derives knowledge based output , , retrains model using obtained knowledge .\n",
      " - DeepCloak method DeepCloak method enhances robustness dnn model removing useless feature [ 27 ] .\n",
      " - first , method insert masking layer network decision layer .\n",
      " - , added layer trained using original adversarial image pair .\n",
      " - finally , feature difference decision layer added layer encoded .\n",
      " - appending external model GAN - based defense method Goodfellow et al .\n",
      " - first proposed intriguing DL model , i.e .\n",
      " - , Generative Adversarial Networks ( gans ) , enhance semantically segment imagery [ 29 ] .\n",
      " - used popular GAN platform construct robust model resists FGSM attack [ 44 ] .\n",
      " - based GAN training , deployed model successfully classify original image , well contaminated image ( Fig .\n",
      " - moreover , motivated concept gans , Shen et al .\n",
      " - presented novel GAN - based method , generator part GAN used rectify perturbed image [ 85 ] .\n",
      " - presented novel approach detect mali- cious perturbation image via feature squeezing [ 102 ] .\n",
      " - , compare model classi- fication accuracy obtained original image compressed image .\n",
      " - framework feature squeezing method [ 102 ] .\n",
      " - depending upon PRN , rectified input data origin input data characterized distribution [ 2 ] .\n",
      " - PRN pre - input layer trained without change parameter targeted network .\n",
      " - 13 , data rectification process , method first feed input image PRN , , detects possible perturbation based output feature PRN .\n",
      " - summary , existing defense measure reported successful resist adversarial example ML predic- tion phase extend .\n",
      " - , [ 57,62,78,84 ] ) provide favorable result kind specific adversarial attack , whereas method usually vulnera- ble adversarial example due transferability .\n",
      " - privacy protection ML model construct effective ML model , large amount training data required .\n",
      " - generally , training data collected using crowd sourcing technique .\n",
      " - ensure data privacy ML model , two popular de- fense strategy usually employed , i.e .\n",
      " - , deferential privacy ( dp ) -based method homomorphic encryption ( ) -based method .\n",
      " - deferential - privacy - based method dwork et al .\n",
      " - proposed Deferential privacy ( dp ) technique 2006 proved robustness security the- ory [ 20 ] .\n",
      " - dp technique widely accepted mean ensuring model privacy , therein aiming obscuring input adding noise original data model .\n",
      " - proposed randomized aggregative privacy - preserving ordinal response ( rappor ) method achieve strong security crowd sourcing process user terminal .\n",
      " - method combine randomized response mechanic dp technique obtain high performance .\n",
      " - proposed promising approach , PATE ( Private Aggregation Teacher en- sembles ) [ 70 ] , ensure privacy ML model .\n",
      " - method construct teacher model training disjoint subset data formulating student model based out- put teacher system .\n",
      " - note training process student model depends prediction result teacher model , instead intrinsic parameter .\n",
      " - homomorphic - encryption - based method another important technique ML data privacy protection Homomorphic encryption ( ) based method .\n",
      " - special cryptography technology allows certain algebraic operation ciphertexts [ 76 ] .\n",
      " - specifically , encryption value algebraic operation result plaintexts consistent algebraic operation result corresponding ciphertexts .\n",
      " - framework universal perturbation defense method [ 2 ] .\n",
      " - technique , substantial amount important data securely transmitted cloud environment [ 53,54 ] .\n",
      " - proposed distributed k - mean crusting algorithm using full model .\n",
      " - developed neural - like network , CryptoNets , encrypting model parameter validated effectiveness MNIST dataset [ 19 ] .\n",
      " - paper presented comprehensive survey security ML model adversarial setting .\n",
      " - future work ML security may include following aspect .\n",
      " - ( 1 ) defense strategy adversarial example DL method include attention .\n",
      " - recent year , many study demonstrated dnns vulnera- ble subtle input perturbation result substantial change output .\n",
      " - ( 2 ) data privacy important ML security .\n",
      " - hence , worth designing effec- tive efficient encryption method ensure ML model privacy .\n",
      " - continuous emergence new security threat , ML designer place greater importance security evalua- tion ML model .\n",
      " - however , evaluation method exploration discussion .\n",
      " - therefore , establishing additional integrity authority evaluation mechanic urgent problem must solved .\n",
      " - Tygar , security machine learning , Mach .\n",
      " - cybernet .\n",
      " - Flynn , data clustering : review , acm comput .\n",
      " - [ 40 ] a. krizhevsky , learning multiple layer feature Tiny Images , label noise , J. Mach .\n",
      " - [ 10 ] b. biggio , I. Pillai , d. ariu , M. Pelillo , F. Roli , data clustering adversarial setting secure ?\n",
      " - [ 11 ] b. biggio , I. Pillai , s. rota bul , d. ariu , M. Pelillo , F. Roli , data clustering adversarial setting secure ?\n",
      " - manzagol , p. vincent , S. Bengio , unsupervised pre - training help deep learning ?\n",
      " - Goodfellow , J. Pougetabadie , M. Mirza , B. Xu , d. wardefarley , S. Ozair , A. Courville , Y. Bengio , generative adversarial network , Adv .\n",
      " - Goodfellow , adversarial example , Comput .\n",
      " - Dumais , E. Osuna , J. Platt , b. scholkopf , support vector machine , IEEE Intell .\n",
      " - Jackel , backpropagation applied handwritten zip code recognition , Neural Comput .\n",
      " - [ 46 ] t. li , C. Gao , L. Jiang , w. pedrycz , J. Shen , publicly verifiable privacy- preserving aggregation application IoT , J. Netw .\n",
      " - [ 47 ] h. li , H. Liu , X. Ji , G. Li , L. Shi , cifar10 - dvs : event - stream dataset [ 48 ] object classification , Front .\n",
      " - Jie , dynamic access policy cloud- inform .\n",
      " - [ 56 ] y. luo , X. Boix , G. Roig , T. Poggio , Q. Zhao , foveation - based mechanism alleviate adversarial example , Comput .\n",
      " - power , evaluation : precision , recall f - factor ROC , informedness , markedness & correlation , J. Mach .\n",
      " - [ 73 ] L. Qin , Y. Guo , W. Jie , G. Wang , effective query grouping strategy cloud , J. Comput .\n",
      " - Huang , application speech - - text recognition computer - aided translation facilitating cross - cultural learning learning activity : issue solution , educat .\n",
      " - [ 84 ] u. shaham , Y. Yamada , S. Negahban , understanding adversarial training : increasing local stability neural net robust optimization , comput .\n",
      " - Hughes , RNA splicing .\n",
      " - [ 105 ] q. zhang , Y. Gan , L. Liu , X. Wang , X. Luo , Y. Li , authenticated asymmetric group key agreement based attribute encryption , J. Netw .\n",
      " - Tan , packet - reordering covert channel VoLTE voice video traffic , J. Netw .\n",
      " - research interest include deep learning , image processing understanding .\n",
      " - research interest include cloud computing , applied cryptography privacy - preserving , etc .\n",
      " - main research interest include Cyberspace Security network security test evaluation .\n",
      " - main research interest include network storage , information security , embedded system .\n",
      "\n",
      "Cluster 0:\n",
      " - however , assumption usually hold sense ML model designed adversarial setting , statistical property data tampered capable adversary .\n",
      " - specifically , observed adversarial example ( also known ad- versarial input perambulation ) elaborately crafted training / test phase seriously undermine ML performance .\n",
      " - susceptibility ML model adversarial setting corresponding countermeasure studied many researcher academic industrial community .\n",
      " - , review adversarial attack method discus defense strategy .\n",
      " - , natural language under- standing , object recognition , target detection , failure local- ization [ 101 ] .\n",
      " - distinguishing characteristic ML programmer design way discover solution problem using instance solved problem , instead deterministically describing solution via coding .\n",
      " - case whereby ML model designed adversarial setting capable adversary able modify statistical property source data .\n",
      " - contrast , source data adversarial setting neutral , adversarial noise carefully crafted maximize classifier error ( Fig .\n",
      " - also reached point whereby ML performs satisfactorily may easily broken adversarial example carefully crafted toward training test phase [ 31 ] .\n",
      " - revealed interesting property neural network contradict commonly held belief [ 95 ] .\n",
      " - discovery lead quite astonishing view : exists `` blind point `` every deep neural network sense input perturbation imperceivable human eye completely fool ML model .\n",
      " - formulate adversarial example , forcing network produce incorrect prediction high confidence [ 66 ] .\n",
      " - ML method applied case adversary incentive disturb operation ML system , increasing interest studying adversarial attack behavior protection strategy adversarial setting .\n",
      " - es- pecially recent year , discovery adversarial example raised substantial concern various practical application , e.g .\n",
      " - example , area medical imaging [ 25 ] , benign tumor made misclassified malignant tumor ML - based tumor detection system adding carefully designed distortion origin image ( Fig .\n",
      " - PDF malware classification system , PDFrate [ 89 ] hidost [ 91 ] , elaborate maleware variant preserve malicious behavior also classified benign classifier [ 48,103 ] .\n",
      " - addition , adversary even fool automated driving system designing perturbed image look like stop sign human eye detected passing sign ML system [ 32 ] .\n",
      " - briefly introduce taxonomy ML method section 2 give framework ml security adversarial setting analyzing ML security model section 3 .\n",
      " - section 4 review adversarial attack according ML implementation phase , section 5 discus various defense approach adversarial attack .\n",
      " - adversarial setting , various adversarial attack ascertained real world [ 23,41 ] .\n",
      " - main characteristic USL method training input unlabeled .\n",
      " - literature reported USL method also suffer adversarial attack malicious setting practice [ 38,45 ] , although adversarial example mainly designed SL method .\n",
      " - security model machine learning security model ML includes attack surface , threat model adversarial model .\n",
      " - security model provides blueprint studying attack defense mechanic .\n",
      " - formulated system attack surface consisting ML model dataset based adversarials attack objective [ 68 ] .\n",
      " - 3 show attack surface ML system , constructed describe feature available attack adversarial setting .\n",
      " - adversarial example medicine imaging field [ 25 ] .\n",
      " - adversarial example formulated using origin image carefully crafted perturbation benign tumor misclassified malignant tumor 100 % confidence .\n",
      " - attack surface ML .\n",
      " - pipeline ML , whereas bottom line represents various attack ML model suffers according data processing pipeline .\n",
      " - model detection retraining process also compromised stealing poisoning attack .\n",
      " - threat model machine learning systematically researching ML threat model , Dalvi first studied adversarial attack ML - based spam detection system [ 16 ] 2004 .\n",
      " - discussed label leaking phenomenon ML adversarial setting 2016 [ 41 ] .\n",
      " - ( 1 ) direction attack influence , threat model ML divided causative attack exploratory attack .\n",
      " - Causative attack affect ML model controlling training data , exploratory attack produce misclassified result influence training data .\n",
      " - ( 2 ) direction security violation , threat model ML classified integrity attack , availability attack confidentiality attack .\n",
      " - integrity attack compromise model asset via false negative .\n",
      " - integrity ML model compromised , deviation prediction result may generated [ 72 ] .\n",
      " - example , junk email mod- ified good word fool spam filter ( shown Fig .\n",
      " - availability attack usually caused false positive , e.g .\n",
      " - , denial - - service ( do ) attack .\n",
      " - instance , adversary put malicious object difficult identify ML system busy road , autonomous vehicle may forced stop along side road .\n",
      " - confiden- tiality attack attempt steal sensitive information dataset parameter ML model .\n",
      " - well- designed ML system protect important information unauthorized user .\n",
      " - example , ML - based medical diagnosis system required prevent adversary analyzing model recovering information patient .\n",
      " - ( 3 ) direction attack specification , threat model divided discriminate attack indiscriminate attack .\n",
      " - discriminate attack primarily concern particular instance , whereas indiscriminate attack encompass broad range instance .\n",
      " - ( 1 ) adversarial goal .\n",
      " - adversarial goal defined two perspective , i.e .\n",
      " - , level destruction caused adversary attack specificality , discussed section .\n",
      " - former , adversary impact confidentiality , integrity , availability privacy ML model latter , discriminate non- discriminate attack produced .\n",
      " - example , adver- sary maximize misclassification probability ML system using discriminate integrity attack also obtain confidential information guest via targeted attack .\n",
      " - taxonomy threat model ML .\n",
      " - adversarial capability [ 64 ] .\n",
      " - ( 2 ) adversarial knowledge .\n",
      " - adversarial ( 3 ) adversarial capability .\n",
      " - term capability refers behavior available attack .\n",
      " - behavior depends possible attack vector threat surface .\n",
      " - prediction / test phase , adversary observe model behavior deduce system vulnerability ( also known black - box ) steal internal model information ( also known white - box ) design adversarial example .\n",
      " - training phase , adversary use / read / write access mimic corrupt training sample .\n",
      " - attack strategy refers method adversary modify training testing dataset optimize attack .\n",
      " - adversarial attack research ml security mainly focus supervised ML method .\n",
      " - many adversarial attack discovered early supervised - based security detection system .\n",
      " - proposed inject malicious data NB - based spam de- tection system obfuscate model prediction [ 99 ] .\n",
      " - broke SVM - based malicious PDF detection system crafting malicious PDF variant [ 91,104 ] .\n",
      " - addition , unsupervised ML method also face security issue adversarial setting [ 11,12,50,108 ] .\n",
      " - instance , adversary inject malicious example clustering sys- tem impact clustering result .\n",
      " - moreover , obfuscation attack hide adversarial example keeping clustering result unchanged [ 10 ] .\n",
      " - interesting finding lead astonishing aspect : exist special input close correctly classified sample thoroughly misclassified model .\n",
      " - proposed construct adversarial example take form small perturbation input image deceive dnn model [ 95 ] .\n",
      " - adversarial attack training phase adversary attempt compromise ML model attacking original data ML training phase .\n",
      " - poi- soning attack typical type attack attempt modify statistical characteristic training dataset .\n",
      " - poisoning 16 X. Wang , J. Li , X. Kuang et al .\n",
      " - attacking phase black / white box learning method adversarial goal poisoning l - bfgs FGSM UAP upset / angr c & w training predicting predicting predicting predicting predicting white box white box white box white box black box white box one shot one shot Iterative Iterative Iterative iterative integrity availability availability availability availability availability make prediction data training dataset .\n",
      " - extensive application DL method various field , defense strategy adversarial attack prediction / test phase developed based dnn structure .\n",
      " - section review typical method attempt fool dnns .\n",
      " - observed imperceptive perturbation image fool dl model providing misclassifi- Rm clean image cation [ 95 ] .\n",
      " - derive additive perturbation Rm slightly distorts image fool network , following problem must addressed .\n",
      " - method creates adversarial example appending noise original image along gradient direction .\n",
      " - FGSM adversarial example derived via following equation .\n",
      " - 7 demonstrates example FGSM adversarial ImageNet dataset .\n",
      " - universal adversarial perturbation ( UAP ) FGSM l - bfgs method employ adversarial exam- ples attack dnn model using single image .\n",
      " - at- tack exhibiting promising result one image usually fail achieve favorable result image .\n",
      " - developed universal attack ap- proach universal adversarial perturbation generated [ 58 ] .\n",
      " - illustration poisoning attack .\n",
      " - attack regarded causative attack break integrity availability ML model .\n",
      " - case , original dataset used ML system confidential easily modified attacker .\n",
      " - , bio- metric facial recognition , malware detection spam email filtering ) , ML model usually required retrained be- cause training dataset may degenerate environment change .\n",
      " - fact provides opportunity attacker manipulate ML training data .\n",
      " - 6 sketch poison- ing attack .\n",
      " - observed feature centroid ML model transferred unexpected position injecting malicious example original training dataset .\n",
      " - generate malicious example poisoning attack , gen- eral approach obscure label training dataset .\n",
      " - according label obscuring pattern , poisoning attack classified random label flipping ( rlf ) attack , nearest- prior label flipping ( NPLF ) attack , farthest - prior label flipping ( fplf ) attack , farthest - rotation label flipping ( FRLF ) attack adversarial label flipping ( ALF ) attack [ 1,9 ] .\n",
      " - rlf attack randomly select training data modify label altered label lead misclassifications output .\n",
      " - NNPLF attack distort data label near decision boundary ML model .\n",
      " - attack method result complex decision boundary .\n",
      " - contrast , FRLF attack tamper data label far ML de- cision boundary .\n",
      " - proposed novel FRLF attack , training data far SVM decision boundary random hyperplane chosen example modified .\n",
      " - contrast fplf flipping attack , hyperplane trained modified example obtain larger angle .\n",
      " - presented new ( ALF ) attack attempt maximum classification error ML mode trained distorted example [ 100 ] .\n",
      " - adversarial attack prediction / test phase prediction / test phase process yield inference based trained model .\n",
      " - adversarial example created FGSM method ImageNet [ 96 ] .\n",
      " - upset ANGR method two block - box attack method , upset ANGR , pro- posed Sarkar et al .\n",
      " - DL model n class , upset method attempt generate k image - unconvinced perturbation , appending perturba- tions image untargeted class , network still classify distorted image corresponding target class .\n",
      " - C & W attack method Carlini Wagner introduced powerful adversarial at- tack , also known C & W attack , achieves impres- sive result distilled undistilled dnn model [ 15 ] .\n",
      " - attack apply three distance measure , i.e .\n",
      " - moreover , also observed adversarial example generated unsecured dnn model effectively transfer secured model , enabling derived adversarial example proper black - box attack .\n",
      " - 1 summary , existing attack method prohibit impres- sive result fooling dnn model various datasets , e.g .\n",
      " - demon- strates adversarial attack general phenomenon field ML .\n",
      " - many literature ascertains DL method also vulnerable adversarial attack real physical world .\n",
      " - however , although many hypothesis developed explain existence adversarial example [ 65,82,90 ] , underlying reason fact still open question .\n",
      " - security evaluation mechanism traditional security evaluation method primarily focus classification accuracy ML model , fail consider security model .\n",
      " - motivation security evaluation simulate various realistic attack scenario highlight critical vulnerability analyzing influence adversarial attack given ML model .\n",
      " - reactive defense refers arm race ML model designer adversary involved .\n",
      " - iteration , adversary first analyzes ML defense strategy develops attack approach model .\n",
      " - , designer ML model responds action based proposed attack type .\n",
      " - contrast reactive defense , main agent arm race proactive defense designer ML model .\n",
      " - deploying ML system , designer develops defense strategy analyzing possible deficiency threat ML model based adversarial capability , attack strategy , etc .\n",
      " - defense strategy training phase ML training phase , adversary deploy poisoning attack modify statistical characteristic training data .\n",
      " - data sanitization technique immediate method , therein aiming filter adversarial example applied training dataset .\n",
      " - proposed novel method reject negative impact spam filtering system via data sanitiza- tion [ 61 ] .\n",
      " - ( 2 ) external model , method use additional module detect suppress adversarial attack , thereby keeping ML model parameter unchanged .\n",
      " - modifying ML model adversarial training method adversarial training first approach adversar- ial attack .\n",
      " - however , non - adaptive strategy sense ML model required trained relevant adversarial example resist partic- ular type attack .\n",
      " - moreover , new study suggests adversarially trained model attacked effective adversarial example [ 80 ] .\n",
      " - according study , jpg compression suppress decrease performance large extent term classification accuracy FGSM perturba- tions .\n",
      " - method employ ensemble- based technique counter FGSM attack removing high - frequency component image .\n",
      " - main limitation data compression strategy higher compression rate lead loss ML classification accuracy , whereas low compression rate effectively counter adversarial attack .\n",
      " - demonstrated influence adversarial attack mitigated foveation mechanism differ- ent image region [ 56 ] .\n",
      " - result , foveation used alternative solution alleviating impact adversarial attack .\n",
      " - error rate two datasets considerable difference , candidate sample identified malicious example required removed .\n",
      " - relevant experiment conducted spam filter system , corresponding result illustrate proposed approach effectively detect malicious data .\n",
      " - one limitation work method appropriate large - scale data heavy computational cost .\n",
      " - method employ median absolute deviation Laplace truncated domain minimize influence malicious training data .\n",
      " - result experiment show method effective suppressing malicious data .\n",
      " - defense strategy prediction / test phase defense adversarial attack prediction / test phase follow two main direction : ( 1 ) modifying ml model , e.g .\n",
      " - illustration defensive distillation strategy adversarial attack [ 36 ] .\n",
      " - proposed mitigate l - bfgs FGSM attack penalizing gradient loss function network [ 57 ] .\n",
      " - moreover , Nguyen sinha developed gradient masking method defend C & W attack , noise appended network logit layer [ 62 ] .\n",
      " - small adversarial perturbation drastically vary output deployed model , method train model penalizing input variation degree .\n",
      " - formulated distillation variant , defensive distillation , resist adversarial perturbation used attack dnn model [ 69 ] .\n",
      " - contrast knowledge transferring pattern original distillation , defensive distillation method extract knowledge structure enhance resistance adversarial attack .\n",
      " - prominent feature dominant weight , prominent feature removed masking dominant weight added layer .\n",
      " - compared - mentioned defense algorithm , DeepCloak method efficient eliminates unnecessary fea- tures without retraining model .\n",
      " - method first decrease color depth image external module .\n",
      " - exist substantial difference two accuracy , target image regarded adversarial example .\n",
      " - universal perturbation defense method goal universal perturbation defense strategy rectify contaminated input perturbation rectifying network ( prn ) input layer .\n",
      " - 13 demon- strates framework universal perturbation defense method based PRN .\n",
      " - , adversarial training [ 30,60,80 ] , data compression [ 17,21,33,59 ] , defensive dis- tillation [ 36,69 ] ) validate defend different kind adversarial attack via carrying extensive experiment .\n",
      " - however , method probably decrease predictive accuracy rate benign example .\n",
      " - therefore , devising robust universal defense strategy absence reducing performance origin ML model challenging task .\n",
      " - training data usually contain large amount private sensitive informa- tion regard user , ensuring training data privacy become crucial issue ensuring security ML technolo- gy .\n",
      " - addition , investigation demonstrated important parameter ML model may also hidden .\n",
      " - instance , adversary perform inversion attack ML system obtain private data inside ML model [ 97 ] .\n",
      " - despite impressive performance ML method achieved , observed ML method highly vulnerable adversarial attack training prediction phase .\n",
      " - although many defense strategy proposed resist attack , find fun- damental solution problem .\n",
      " - therefore , developing secure DL model adversarial setting crucial issue .\n",
      " - although dp technique provide powerful guarantee ML method , remain inefficient high com- putational complexity .\n",
      " - ( 3 ) security evaluation ML method adversarial setting highlighted research .\n",
      " - conflict interest author associated paper disclosed po- tential pertinent conflict may perceived impending conflict work .\n",
      " - [ 7 ] b. biggio , G. Fumera , F. Roli , security evaluation pattern classifier attack , IEEE Trans .\n",
      " - [ 8 ] b. biggio , G. Fumera , F. Roli , security evaluation pattern classifier attack , IEEE Trans .\n",
      " - [ 9 ] b. biggio , B. Nelson , P. Laskov , support vector machine adversarial [ 37 ] hu , junling , wellman , P. Michael , Nash q - learning general - sum stochastic game , J. Mach .\n",
      " - Tygar , k. xia , Misleading Learners : co - opting spam filter , 2009 , pp .\n",
      " - Celik , A. Swami , prac- tical Black - Box Attacks Deep Learning Systems using Adversarial Examples , 2016 .\n",
      " - Wu , attacking statistical spam filter , : conference email & anti - spam , 2004 .\n",
      "\n",
      "Cluster 1:\n",
      " - 2019 Elsevier Inc. right reserved .\n",
      " - based data characteristic , ML meth- od classified three category [ 77 ] , i.e .\n",
      " - 2010 , Barreno et al .\n",
      " - [ 73,74 ] .\n",
      " - X. Wang , J. Li , X. Kuang et al .\n",
      " - revealed two characteristic regard dnns work [ 95 ] .\n",
      " - first observation refers fact single feature dnn uninterpretable .\n",
      " - , navigational system [ 52,60,63,67 ] .\n",
      " - l - bfgs method szegedy et al .\n",
      " - ( 1 ) subject h ( ic + ic + [ 0 , 1 ] .\n",
      " - note data employed X. Wang , J. Li , X. Kuang et al .\n",
      " - , l , l2 , l0 .\n",
      " - , mnist [ 43 ] , ImageNet [ 18 ] cifar-10 [ 47 ] .\n",
      " - mean distillation decrease dimensionality dnn .\n",
      " - flowchart DeepCloak illustrated Fig .\n",
      " - 20 X. Wang , J. Li , X. Kuang et al .\n",
      " - defense strategy based GAN [ 29 ] .\n",
      " - based dp , erlings- son et al .\n",
      " - 14 present overview pate .\n",
      " - X. Wang , J. Li , X. Kuang et al .\n",
      " - overview pate algorithm [ 70 ] .\n",
      " - 2018cxgc0701 ) jsps kakenhi grant .\n",
      " - jp15k00028 .\n",
      " - full disclosure statement refer http : //doi.org/10.1016/j.jpdc.2019.03.003 .\n",
      " - 11001109 .\n",
      " - [ 3 ] g. alvareznarciandi , J. Laviada , M.R .\n",
      " - Antennas propagation 66 ( 99 ) ( 2018 ) 11 .\n",
      " - [ 4 ] m. barreno , B. Nelson , A.D. Joseph , J.D .\n",
      " - 81 ( 2 ) ( 2010 ) 121148 .\n",
      " - 22 X. Wang , J. Li , X. Kuang et al .\n",
      " - 1 ( 14 ) ( 2010 ) 2741 .\n",
      " - 26 ( 4 ) ( 2014 ) 984996 .\n",
      " - 26 ( 4 ) ( 2014 ) 984996 .\n",
      " - 4 ( 4 ) ( 2003 ) 10391069 .\n",
      " - [ 38 ] B. Hu , Y. Tang , I.C .\n",
      " - PP ( 99 ) ( 2017 ) 11 .\n",
      " - [ 39 ] A.K .\n",
      " - Jain , M. Murty , P.J .\n",
      " - 31 ( 3 ) ( 1999 ) 264323 .\n",
      " - 20 ( 3 ) ( 2011 ) 97112 .\n",
      " - 8798 .\n",
      " - 8798 .\n",
      " - 2736 .\n",
      " - [ 14 ] M.N .\n",
      " - 3957 .\n",
      " - 99108 .\n",
      " - [ 17 ] n. da , m. shanbhogue , S.T .\n",
      " - Chen , F. Hohman , L. Chen , M.E .\n",
      " - J. Deng , W. Dong , R. Socher , L.J .\n",
      " - Li , K. Li , F.F .\n",
      " - cvpr 2009 .\n",
      " - 248255 .\n",
      " - [ 18 ] [ 19 ] n. dowlin , G.B .\n",
      " - 7678 .\n",
      " - [ 21 ] G.K. Dziugaite , Z. Ghahramani , D.M .\n",
      " - [ 22 ] d. erhan , Y. Bengio , A. Courville , p.-a .\n",
      " - 11 ( feb ) ( 2010 ) 625660 .\n",
      " - [ 25 ] S.G. Finlayson , I.S .\n",
      " - 3 ( 2014 ) 26722680 .\n",
      " - ( 2014 ) .\n",
      " - 6279 .\n",
      " - [ 33 ] c. guo , M. Rana , m. cisse , l.v.d .\n",
      " - [ 34 ] k. , X. Zhang , s. ren , J .\n",
      " - 770778 .\n",
      " - [ 35 ] m.a .\n",
      " - hearst , S.T .\n",
      " - 13 ( 4 ) ( 1998 ) 1828 .\n",
      " - [ 43 ] Y. Lecun , B. Boser , J.S .\n",
      " - Denker , D. Henderson , R.E .\n",
      " - Howard , W. Hubbard , L.D .\n",
      " - 1 ( 4 ) ( 2014 ) 541551 .\n",
      " - 126 ( 2019 ) 3944 .\n",
      " - 11 ( 2017 ) 309 .\n",
      " - ( 2018 ) .\n",
      " - [ 50 ] c. liang , Y.-a .\n",
      " - 118 ( 2018 ) 144153 .\n",
      " - [ 51 ] c. liang , X. Wang , X. Zhang , Y. Zhang , K. Sharif , Y.-a .\n",
      " - 465 ( 2018 ) 162173 .\n",
      " - 59 ( 2016 ) .\n",
      " - 641647 .\n",
      " - ( 2015 ) .\n",
      " - [ 57 ] c. lyu , K. Huang , H.N .\n",
      " - 301309 .\n",
      " - [ 58 ] s.m .\n",
      " - 8694 .\n",
      " - [ 59 ] S.M .\n",
      " - 8694 .\n",
      " - [ 60 ] S.M .\n",
      " - 25742582 .\n",
      " - [ 61 ] b. nelson , M. Barreno , F.J. Chi , A.D. Joseph , b.i.p .\n",
      " - Rubinstein , u. saini , C. Sutton , J.D .\n",
      " - 1751 .\n",
      " - 427436 .\n",
      " - [ 66 ] n. papernot , P. Mcdaniel , I. Goodfellow , S. Jha , Z.B .\n",
      " - [ 67 ] n. papernot , P. Mcdaniel , S. Jha , M. Fredrikson , Z.B .\n",
      " - 372387 .\n",
      " - [ 36 ] G. Hinton , O. Vinyals , J .\n",
      " - 14 ( 7 ) ( 2015 ) 3839 .\n",
      " - X. Wang , J. Li , X. Kuang et al .\n",
      " - 582597 .\n",
      " - [ 72 ] d.m .\n",
      " - 2 ( 2011 ) 22293981 .\n",
      " - 32 ( 6 ) ( 2017 ) 12311249 .\n",
      " - 28 ( 5 ) ( 2017 ) 14171429 .\n",
      " - [ 75 ] J.R. Quinlan , induction decision tree , Mach .\n",
      " - 1 ( 1 ) ( 1986 ) 81106 .\n",
      " - [ 76 ] R.L .\n",
      " - Rivest , L. Adleman , M.L .\n",
      " - ( 1978 ) 169179 .\n",
      " - [ 79 ] b.i.p .\n",
      " - Rubinstein , B. Nelson , L. Huang , A.D. Joseph , S.H .\n",
      " - Lau , S. Rao , N. Taft , J.D .\n",
      " - [ 80 ] s. sankaranarayanan , A. Jain , R. Chellappa , S.N .\n",
      " - [ 83 ] r. shadiev , T.T .\n",
      " - Wu , a. sun , Y.M .\n",
      " - 66 ( 1 ) ( 2018 ) 124 .\n",
      " - ( 2015 ) .\n",
      " - 453 ( 2018 ) 186197 .\n",
      " - [ 87 ] d. silver , A. Huang , C.J .\n",
      " - Maddison , a. guez , L. Sifre , G. Van Den Driessche , J. Schrittwieser , I. Antonoglou , V. Panneershelvam , M. Lanctot , et al .\n",
      " - 7180 .\n",
      " - 239248 .\n",
      " - 362363 .\n",
      " - [ 93 ] R.S .\n",
      " - ( 2015 ) 28182826 .\n",
      " - ( 2013 ) .\n",
      " - [ 97 ] f. tramr , F. Zhang , A. Juels , M.K .\n",
      " - [ 99 ] g.l .\n",
      " - 870875 .\n",
      " - [ 101 ] H.Y .\n",
      " - Xiong , B. Alipanahi , L.J .\n",
      " - Lee , H. Bretschneider , D. Merico , R.K. Yuen , Y. Hua , S. Gueroussov , H.S .\n",
      " - Najafabadi , T.R .\n",
      " - 123 ( 2018 ) 110 .\n",
      " - 72 ( 2016 ) S0167739X16301170 .\n",
      " - [ 107 ] X. Zhang , L. Zhu , X. Wang , C. Zhang , H. Zhu , Y.-a .\n",
      " - ( 2018 ) .\n",
      " - [ 108 ] W. Zhao , J .\n",
      " - 222233 .\n",
      " - [ 109 ] h. zhu , Y.-a .\n",
      " - Xianmin Wang received B.S .\n",
      " - Jing Li received B.S .\n",
      " - currently , work Guangzhou University .\n",
      " - received B.S .\n",
      " - ( 2002 ) M.S .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cluster, sentences in clusters.items():\n",
    "    print(f\"Cluster {cluster}:\")\n",
    "    for sentence in sentences:\n",
    "        print(f\" - {sentence}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecfffc2-fc3e-4729-9217-7b67a038b24c",
   "metadata": {},
   "source": [
    "## Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9aa96767-3858-4eb4-abcb-733db206af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ffb0305f-4d48-4b54-8c0d-bb7bd95020fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API Key: ········\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = getpass('Enter OpenAI API Key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dca875da-d910-44a8-8859-4b500172b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c8e630de-758c-4695-8140-a814d6c0582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstractive_summary(clusters):\n",
    "    summary = []\n",
    "    for cluster in clusters.values():\n",
    "        \n",
    "        cluster_text = ' '.join(cluster)\n",
    "        chunk_summary = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': f\"Summarize the following text:\\n{cluster_text}\"\n",
    "                }\n",
    "            ],\n",
    "            model='gpt-3.5-turbo'\n",
    "        )\n",
    "        summary.append(chunk_summary.choices[0].message.content.strip())\n",
    "\n",
    "    summary_text = ' '.join(summary)\n",
    "    final_summarization = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f\"Improve the semantic format:\\n{summary_text}\"\n",
    "            }\n",
    "        ],\n",
    "        model='gpt-3.5-turbo'\n",
    "    )\n",
    "        \n",
    "    return final_summarization.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b32260a1-1b33-458a-8e3a-0f0039ce2de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstractive Summary:\n",
      "The survey explores security in machine learning within adversarial settings, spanning applications like autopilot systems and facial recognition. It delves into attack and defense methods in machine learning security, crediting various funding sources. The authors, with backgrounds in computer science and information security from Chinese universities, focus on secure protocols and privacy protection in new computing environments. They detail defense approaches against adversarial attacks, referencing researchers like Szegedy, Lowd, Kurakin, and Wittel who have proposed techniques such as feature squeezing and foveation-based methods. Papers by Nelson, Guo, and Moosavi are cited for improving model security and robustness. Emphasizing the significance of parallel distributed computing in machine learning research, the text scrutinizes the traditional training and deployment of ML models in benign environments. It investigates the security properties of ML algorithms in adversarial contexts and foresees future work in designing secure models. The discussion includes aspects like attack surfaces, defense strategies, adversarial examples, and data privacy protection techniques, with specific recommendations like adversarial training and data compression to bolster model robustness. Future research directions are suggested for defense strategies against adversarial examples in deep learning and encryption methods for maintaining model privacy. The vulnerability of ML models to adversarial attacks is underscored, with various attack methods and defense strategies examined across different ML phases. The text stresses the necessity of security evaluation in ML models and the challenges of developing strong defense strategies while upholding model performance. Privacy concerns and hidden parameters in ML models are also addressed, underscoring the demand for secure deep learning models and ongoing research efforts in this domain. Methodologies and research papers in machine learning, including data characteristics and classification techniques, are discussed, highlighting deep neural networks and defense strategies utilizing generative adversarial networks. References to studies and papers, including the work of authors Xianmin Wang and Jing Li, are provided.\n"
     ]
    }
   ],
   "source": [
    "summary = abstractive_summary(clusters)\n",
    "print(\"Abstractive Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c61dd10-7ba1-4418-abd9-71c98dcb3072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
