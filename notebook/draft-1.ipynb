{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bb3135-5926-454e-ac97-a872bf12b9ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ec842-ebc1-4cac-8aba-69cd91ceb864",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e99d622e-2658-4d39-a545-ff1707e3afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Extraction\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Embedding Generation\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Semantic Chunking\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# LLM for summarization\n",
    "import os\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca3573-e78e-4f93-9b0f-5016291a0950",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Module Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5861107-d112-4f1b-8312-628dd4d0f3d2",
   "metadata": {},
   "source": [
    "### Preprocessing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a822b5f-e0b5-4b44-bcad-6c842786da28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f6a59-47ad-4824-b746-1a68561efcc5",
   "metadata": {},
   "source": [
    "### Tokenization Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a122712d-ad31-4881-ab5c-c1c677226eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e284696a-7413-45b1-b8ae-d1022f4a3a6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LLM API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0082f217-6238-4f6d-aa0a-3bbc45afc0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API Key: ········\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = getpass('Enter OpenAI API Key:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f795a026-09a8-4792-9cc4-910f18ebd2c3",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c90f56-4509-4ea1-b7c4-c59fc7e8f251",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e7cbe-aa54-4650-bd7e-4745a41e3a0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e4041d2-ed15-4adc-9981-9ed0dfbb344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_text_from_pdf(filepath):\n",
    "    text = extract_text(filepath)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d0fb17-bc67-41ce-8716-aac1d00d1b2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f66f3bda-7d90-4dac-b874-b54ed7d6645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_text(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    processed_tokens = []\n",
    "    for token in doc:\n",
    "        if token.ent_type_ in ('PERSON', 'ORG', 'GPE', 'LOC'):\n",
    "            processed_tokens.append(token.text)\n",
    "        else:\n",
    "            processed_tokens.append(token.text.lower())\n",
    "\n",
    "    text = ' '.join(processed_tokens)\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    preprocessed_text = ' '.join(words)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802595ff-8b0a-4388-8619-1a27d9cd3907",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15c0183c-7d48-47db-9381-591a8010a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _embedding_generation(preprocessed_text):\n",
    "    sentences = sent_tokenize(preprocessed_text)\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "    return sentence_embeddings, sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f838298b-eb21-4134-986c-6abaf8c19706",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "120723e7-3fdc-43b3-b6ac-aba8ab7a4ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _semantic_chunking(preprocessed_text, n_clusters):\n",
    "    sentence_embeddings, sentences = _embedding_generation(preprocessed_text)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters, n_init='auto', random_state=42)\n",
    "    kmeans.fit(sentence_embeddings)\n",
    "\n",
    "    clusters = {}\n",
    "    for idx, label in enumerate(kmeans.labels_):\n",
    "        if label not in clusters:\n",
    "            clusters[label] = []\n",
    "            clusters[label].append(sentences[idx])\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c2052-8afc-4b6f-83d8-80750055211e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d25fd33-afc2-460f-97e8-c0b1a1b1d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _abstractive_summary(clusters):\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    summary = []\n",
    "    for cluster in clusters.values():\n",
    "        \n",
    "        cluster_text = ' '.join(cluster)\n",
    "        chunk_summary = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': f\"Provide a well detailed technical summary of the following text:\\n{cluster_text}\"\n",
    "                }\n",
    "            ],\n",
    "            model='gpt-3.5-turbo'\n",
    "        )\n",
    "        summary.append(chunk_summary.choices[0].message.content.strip())\n",
    "\n",
    "    summary_text = ' '.join(summary)\n",
    "    final_summarization = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f\"Improve the semantic format:\\n{summary_text}\"\n",
    "            }\n",
    "        ],\n",
    "        model='gpt-3.5-turbo'\n",
    "    )\n",
    "        \n",
    "    return final_summarization.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e279259-796d-43bf-8a73-1e4d8ea63484",
   "metadata": {},
   "source": [
    "## Executable Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf93b333-05bf-4c54-8162-28e095013b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pdf(filepath, n_clusters=5):\n",
    "    text = _extract_text_from_pdf(filepath)\n",
    "    preprocessed_text = _preprocess_text(text)\n",
    "    semantically_chunked_text = _semantic_chunking(preprocessed_text, n_clusters)\n",
    "    summary = _abstractive_summary(semantically_chunked_text)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c686950-765f-4650-90d4-fccd8dc378a8",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9dcdb59-39a0-44fc-9ec9-0f303ed27fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'PDFs/2.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b76e3f22-f331-4c5d-98c4-78fd4f58bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize_pdf(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59ef3c32-ff66-4814-98e1-c399a02b1c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text provides a summary of a survey conducted by researchers from various universities in the UK and Denmark on attack methodologies in machine learning. The article was received in December 2018, revised in June 2019, and accepted for publication in September 2019. It was made available online in October 2019.\n",
      "\n",
      "The researchers discuss machine learning methodologies that assume a benign environment and present a taxonomy of attacks in machine learning systems. The study aims to provide an overview of attack techniques and strategies used in the field of machine learning.\n",
      "\n",
      "Researchers from multiple universities conducted the study, focusing on vulnerabilities and threats faced by machine learning systems. The text addresses potential vulnerabilities in machine learning models and discusses poisoning attacks, where false data is inserted into the training set to bias the model, and evasion attacks, where test data is manipulated to evade detection by the model.\n",
      "\n",
      "It is crucial for developers and researchers to be aware of these attacks and implement robust defenses, such as data sanitization and anomaly detection, to ensure the reliability of machine learning models. Secure machine learning techniques, such as differential privacy and robust optimization, are essential to safeguard machine learning applications against adversarial attacks.\n",
      "\n",
      "In conclusion, developing secure machine learning algorithms is vital for ensuring the safety and reliability of machine learning applications in critical domains. The text \"2019 Elsevier Inc. right reserved\" indicates that the content is copyrighted and owned by Elsevier Inc. for the year 2019, warning against unauthorized use. The text \"2\" is incomplete and lacks context to provide a detailed technical summary.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0eabeb-7068-4a04-9617-56a1898934db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
